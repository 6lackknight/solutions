{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLU Testing for Botpress 12\n",
        "## Now in a Jupyter notebook!\n",
        "\n",
        "This code is designed to do three things:\n",
        "  1. It uses your credentials to generate an access token\n",
        "  2. Takes individual utterances from a test dataset and sends them via converse API to the bot and records the intent\n",
        "    - If no intent is matched, it will record 'No Intent Matched'\n",
        "    - After all testing is done it will save the test results\n",
        "    - Depending on parameters in the .env, extracted slots, confidence, and context are saved\n",
        "  3. It uses the results to create a confusion matrix that's saved in the base folder. Precision, Recall, and F1 scores are also calculated.\n",
        "\n",
        "## To use this notebook:\n",
        "  1. Fill out the \"Test Parameters\" form with the relevant data\n",
        "  2. Go to `Runtime` > `Run All`\n",
        "\n",
        "For the best experience, run this in a Google Colab environment so that all your files are read and written to/from your Google Drive."
      ],
      "metadata": {
        "id": "Q7tBIRhmzdQ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test Parameters { form-width: \"50%\", display-mode: \"both\" }\n",
        "endpoint = \"https://endpoint.botpress.cloud\" #@param {type:\"string\"} \n",
        "#@markdown _The base URL where botpress is exposed_\n",
        "user = \"user@email.com\" #@param {type:\"string\"}\n",
        "#@markdown _The usename or email address you use to login to the above instance_\n",
        "password = \"password123\" #@param {type:\"string\"}\n",
        "#@markdown _The password for the above username_\n",
        "botId = \"myBot\" #@param {type:\"string\"}\n",
        "#@markdown _The Id of the bot you want to test_\n",
        "testName = \"Test_1\" #@param {type:\"string\"}\n",
        "#@markdown _A descriptve label for this test_\n",
        "#\n",
        "extractEntities = True #@param {type:\"boolean\"} \n",
        "#@markdown _Check to extract entities when running tests_\n",
        "extractConfidence = False #@param {type:\"boolean\"}\n",
        "#@markdown _Check to extract confidence when running tests_\n",
        "extractContext = False #@param {type:\"boolean\"}\n",
        "#@markdown _Check to extract context when running tests_\n",
        "\n",
        "testPath = \"MyDrive/NLU-Testing/Software-Test/\" #@param {type:\"string\"}\n",
        "#@markdown _Where in your Google Drive your test folder lives. It should start with \"MyDrive/\"_\n",
        "testFilename = \"Test-Phrases.csv\" #@param {type:\"string\"}\n",
        "#@markdown _The CSV file that contains your test data. The first two columns **must** be Utterance | Expected_\n",
        "isGoogleColab = True #@param{type:\"boolean\"}\n",
        "#@markdown _True if you're running this notebook in a Colab env and need to pull files from Google Drive_\n",
        "\n",
        "callsPerSecond = 15 #@param {type:\"integer\"}\n",
        "#@markdown _The maximum number of converse API calls per second_"
      ],
      "metadata": {
        "id": "S5ZKS4Ez0DsJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install dependencies\n",
        "import requests\n",
        "import pandas\n",
        "import os\n",
        "import json\n",
        "import re\n",
        "import uuid\n",
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sklearn.metrics\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "%pip install ratelimit\n",
        "from ratelimit import limits, RateLimitException, sleep_and_retry\n",
        "from tqdm.notebook import tqdm, trange\n",
        "if isGoogleColab:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTId5Eqk3t5h",
        "outputId": "bb2fc4de-fc98-4ec5-eb14-e29649f1a4a6",
        "cellView": "form"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ratelimit in /usr/local/lib/python3.7/dist-packages (2.2.1)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Definitions\n",
        "def getToken(endpoint, email, password):\n",
        "    '''\n",
        "    Returns a valid authentication token or None in case of an error.\n",
        "\n",
        "            Parameters:\n",
        "                    endpoint (string): The exposed Botpress URL\n",
        "                    email (string): A valid username or email\n",
        "                    password (string): The password for the supplied username or email.\n",
        "\n",
        "            Returns:\n",
        "                    token (string): A JWT token string or None if there was an error.\n",
        "    '''\n",
        "    try:\n",
        "        token = requests.post(f\"{endpoint}/api/v1/auth/login/basic/default\", data={\"email\":email, \"password\":password}).json()[\"payload\"][\"jwt\"]\n",
        "        return token\n",
        "    except:\n",
        "        print('Error retrieving auth token- check your credentials')\n",
        "        return None\n",
        "\n",
        "@sleep_and_retry\n",
        "@limits(calls=callsPerSecond, period=1)\n",
        "def getActual(token, utterance, withEntities, withConfidence, withContext):\n",
        "    '''\n",
        "    Returns the cleaned intent or qna Id that the bot matched a given training phrase to.\n",
        "\n",
        "            Parameters:\n",
        "                    token (string): A valid JWT token.\n",
        "                    utterance (string): The test phrase to send to the bot.\n",
        "                    withEntities (boolean): Whether or not to extract & save entities.\n",
        "                    withConfidence (boolean): Whether or not to extract & save confidence.\n",
        "                    withContext (boolean): Whether or not to extract & save context.\n",
        "\n",
        "            Returns:\n",
        "                    result_dict (dict): A dictionary with the utterance as a key and the returned data in a list as the value.\n",
        "    '''\n",
        "    user = uuid.uuid4()\n",
        "    result_dict = {}\n",
        "    payload= {\n",
        "    \"type\":\"text\",\n",
        "    \"text\":utterance\n",
        "   # \"includedContexts\":[\"hr-bot\", \"it-bot\"]\n",
        "    }\n",
        "    auth= {\"Authorization\": \"Bearer \"+token}\n",
        "    url = f\"{endpoint}/api/v1/bots/{botId}/converse/{user}/secured?include=decision,nlu\"\n",
        "    response = requests.post(url, data=payload, headers=auth)\n",
        "    slots = \"\"\n",
        "    #try finding confidence in intent.confidence\n",
        "    try:\n",
        "        actual = \"No Intent Matched\" if response.json()[\"nlu\"][\"intent\"][\"name\"] == 'none' else response.json()[\"nlu\"][\"intent\"][\"name\"]\n",
        "        extracted = response.json()['nlu']['slots']\n",
        "        #Add entities if desired\n",
        "        if((len(extracted)>0) & withEntities):\n",
        "            slots = [f\"{extracted[i]['name']} was extracted from \\\"{extracted[i]['source']}\\\" and normalized to {extracted[i]['value']}\"\n",
        "                    for i in extracted]\n",
        "        else: entities = None\n",
        "        #Add confidence if desired\n",
        "        if(withConfidence):\n",
        "            conf = response.json()[\"nlu\"][\"intent\"][\"confidence\"]\n",
        "        else: conf = None\n",
        "        if(withContext):\n",
        "            ctxt = response.json()['nlu']['intent']['context']\n",
        "        else: ctxt = None\n",
        "    except:\n",
        "        #print(response.json()['statusCode'])\n",
        "        actual = \"ERROR\"\n",
        "        slots=[]\n",
        "        conf=0\n",
        "\n",
        "    if (re.match(r\"__qna__\",actual)):\n",
        "        actual = actual[18:]\n",
        "    \n",
        "    result_dict[utterance] = [actual]\n",
        "    if(withEntities): \n",
        "        result_dict[utterance].append(str(slots))\n",
        "    if(withConfidence): \n",
        "        result_dict[utterance].append(np.round(conf*100, 2))\n",
        "    if(withContext): \n",
        "        result_dict[utterance].append(ctxt)\n",
        "    return result_dict\n",
        "\n",
        "\n",
        "def runTest(test_df, token):\n",
        "    '''\n",
        "    Loop for concurrent converse API calls.\n",
        "\n",
        "        Parameters:\n",
        "          token (string): A valid auth token.\n",
        "          test_df (DataFrame): A DataFrame with two columns containing the test phrases and their labels.\n",
        "        \n",
        "        Returns:\n",
        "          results (List<DataFrame>): A list of DataFrames that contain the results for a given test phrase.\n",
        "    '''\n",
        "    threads = []\n",
        "    results = []   \n",
        "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
        "      with tqdm(total=len(test_df.index))as pbar:\n",
        "        pbar.set_description(\"Processing...\")\n",
        "        for phrase in test_df.Utterance:\n",
        "            threads.append(executor.submit(getActual, token, phrase, extractEntities, extractConfidence, extractContext))\n",
        "        \n",
        "        for task in as_completed(threads):\n",
        "            results.append(task.result())\n",
        "            pbar.update(1)\n",
        "\n",
        "    return results\n",
        "\n",
        "def generateConfustionMatrix(labels, yPred, yAct, outputPath, includeValues=True, ):\n",
        "  '''\n",
        "    Creates a confusion matrix using matplotlib and also calculates F1 score.\n",
        "\n",
        "        Parameters:\n",
        "          labels (ndarray): A list of all possible test labels.\n",
        "          yPred (List): A list of all the predicted labels.\n",
        "          yAct (List): A list of all the actual ground-truth lables.\n",
        "          outputPath (string): Where to save the final chart, not including the filename.\n",
        "          includeValues (boolean): Whether or not to include numbers in the end confusion matrix.\n",
        "  '''\n",
        "  fig, ax = plt.subplots(figsize=(25,20))\n",
        "  cmp = sklearn.metrics.ConfusionMatrixDisplay.from_predictions(\n",
        "      yAct, \n",
        "      yPred, \n",
        "      labels=labels, \n",
        "      normalize='true', \n",
        "      xticks_rotation='vertical',\n",
        "      #Set to false for no numbers in the confusiton matrix. useful if you have a lot of intents to test\n",
        "      include_values=includeValues, \n",
        "      ax=ax)\n",
        "\n",
        "  #Calculate Scores\n",
        "  f1 = np.around(sklearn.metrics.f1_score(yAct, yPred, labels=labels, average='macro', zero_division=0),4)\n",
        "  prec = np.around(sklearn.metrics.precision_score(yAct, yPred, labels=labels, average='macro',zero_division=0),4)\n",
        "  rec = np.around(sklearn.metrics.recall_score(yAct, yPred, labels=labels, average='macro',zero_division=0),4)\n",
        "  print(f\"------{testName} NLU Scoring--------\\n Precision Score: {prec} \\n Recall Score: {rec} \\n F1 Score: {f1}\")\n",
        "\n",
        "  #Add the scores in the plot's title and save the confusion matrix as a PNG\n",
        "  ax.text(2.5,-1, f\"------{testName} NLU Scoring--------\\n Precision Score: {prec} \\n Recall Score: {rec} \\n F1 Score: {f1}\",\n",
        "  size='large')\n",
        "  fig.tight_layout()\n",
        "  fig.savefig(outputPath + f\"{testName}_confusion_matrix.png\", dpi=fig.dpi)\n",
        "\n",
        "\n",
        "def main():\n",
        "  '''\n",
        "    Main function. Run this one!\n",
        "  '''\n",
        "  token = getToken(endpoint, user, password)\n",
        "  if isGoogleColab:\n",
        "    global testPath",
        "    testPath = \"/content/drive/\"+testPath\n",
        "  test_df = pandas.read_csv(f\"{testPath}{testFilename}\",encoding='utf-8', encoding_errors='replace',on_bad_lines='warn')\n",
        "\n",
        "  if token is None:\n",
        "    print(\"Test aborted due to invalid auth token.\")\n",
        "    return\n",
        "\n",
        "  # Overwrite any previous data with the same labels\n",
        "  if(testName in test_df.columns):\n",
        "      test_df.drop(testName, axis=1, inplace=True)\n",
        "  if( f\"{testName}_entities\" in test_df.columns):\n",
        "      test_df.drop(f\"{testName}_entities\", axis=1, inplace=True)\n",
        "  if(f\"{testName}_confidence\" in test_df.columns):\n",
        "      test_df.drop(f\"{testName}_confidence\", axis=1, inplace=True)\n",
        "  if(f\"{testName}_context\" in test_df.columns):\n",
        "      test_df.drop(f\"{testName}_context\", axis=1, inplace=True)\n",
        "\n",
        "  # Remove any duplicate utterances\n",
        "  test_df.drop_duplicates(subset='Utterance', keep='last', ignore_index=True, inplace=True)\n",
        "\n",
        "  # Start the actual test\n",
        "  print(f\"Starting NLU test.... \\n {test_df.Utterance.size} utterances detected\")\n",
        "  results = runTest(test_df, token)\n",
        "\n",
        "  # Use the global params to get the right list of column names\n",
        "  cols_names = np.ma.masked_array(\n",
        "    [testName, f\"{testName}_entities\", f\"{testName}_confidence\", f\"{testName}_context\"],\n",
        "    mask=~np.array([True, extractEntities, extractConfidence, extractContext]))\n",
        "  \n",
        "  # Combine all results into a single DataFrame and merge it with the test dataset.\n",
        "  results_df = pandas.concat(pandas.DataFrame.from_dict(i, orient='index', columns=cols_names[~cols_names.mask].compressed()) for i in results)\n",
        "  test_df = test_df.merge(right= results_df, how='inner', right_index=True, left_on='Utterance')\n",
        "  # Repeat the test for any errors, checking twice\n",
        "  errors = test_df.loc[(test_df[testName]==None)|(test_df[testName]=='')]\n",
        "  with tqdm(total=len(errors.index))as pbar:\n",
        "    pbar.set_description(\"Checking for errors (1 of 2...)\")\n",
        "    for error in errors.Utterance.index:\n",
        "        test_df.at[error, testName] = list(getActual(test_df.Utterance.iloc[error],extractEntities, extractConfidence, extractContext).values())[0]\n",
        "        pbar.update(1)\n",
        "    pbar.close()\n",
        "  errors = test_df.loc[(test_df[testName]==None)|(test_df[testName]=='')]\n",
        "  with tqdm(total=len(errors.index))as pbar:\n",
        "    pbar.set_description(\"Checking for errors (2 of 2...)\")\n",
        "    for error in errors.Utterance.index:\n",
        "        test_df.at[error, testName] = list(getActual(test_df.Utterance.iloc[error],extractEntities, extractConfidence, extractContext).values())[0]\n",
        "        pbar.update(1)\n",
        "    pbar.close()\n",
        "  test_df.to_csv(f\"{testPath}{testFilename}\", index=False)\n",
        "\n",
        "  generateConfustionMatrix(\n",
        "      labels = np.sort(test_df[testName].unique()),\n",
        "      yPred = test_df[testName].tolist(),\n",
        "      yAct = test_df[\"Expected\"].tolist(),\n",
        "      outputPath = f\"{testPath}\",\n",
        "      includeValues = True)"
      ],
      "metadata": {
        "id": "4k_JyIVy4UFo",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Main Code\n",
        "main()"
      ],
      "metadata": {
        "id": "9l2BjjvjATBq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "outputId": "e5899a96-8aea-455c-b070-96705ba5820a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error retrieving auth token- check your credentials\n",
            "Test aborted due to invalid auth token.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "b'Skipping line 11: expected 1 fields, saw 3\\nSkipping line 14: expected 1 fields, saw 2\\nSkipping line 18: expected 1 fields, saw 2\\n'\n"
          ]
        }
      ]
    }
  ]
}
